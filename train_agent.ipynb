{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "import params\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=self.observation_space.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transform = transforms.Grayscale()\n",
    "        return transform(torch.tensor(np.transpose(observation, (2, 0, 1)).copy(), dtype=torch.float))\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transformations = transforms.Compose([transforms.Resize(self.shape), transforms.Normalize(0, 255)])\n",
    "        return transformations(observation).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x145efe907940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('BreakoutNoFrameskip-v4')\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "env.seed(42)\n",
    "env.action_space.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "env.reset()\n",
    "env.step(1) #Fire starts the ball rolling; need to do this every life\n",
    "for i in range(3):\n",
    "    next_state, reward, done, info = env.step(3)\n",
    "fr = env.render(mode=\"rgb_array\")\n",
    "f, ax = plt.subplots(1,2, figsize=(20, 12))\n",
    "ax[0].imshow(fr)\n",
    "ax[1].imshow(np.sum(next_state.__array__(),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQNAgent\n",
    "import pickle\n",
    "class DDQNAgent:\n",
    "    def __init__(self, action_dim, obs_dim, save_directory, rewards_file):\n",
    "        self.action_dim = action_dim\n",
    "        self.save_directory = save_directory\n",
    "        self.net = DDQNSolver(self.action_dim, obs_dim).cuda()\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_rate_decay = params.exploration_rate_decay\n",
    "        self.exploration_rate_min = params.exploration_rate_min\n",
    "        self.current_step = 0\n",
    "        self.memory = deque(maxlen=100000) #Not sure why, this version does not keep memory on GPU\n",
    "        #But 20 GB gets filled with 100k frames\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95 #Reward decay\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=1e-4)\n",
    "        self.loss = torch.nn.SmoothL1Loss()\n",
    "        self.episode_rewards = []\n",
    "        self.moving_average_episode_rewards = []\n",
    "        self.max_average_episode_rewards = []\n",
    "        self.current_episode_reward = 0.0\n",
    "        self.burn_in = self.batch_size*10\n",
    "        self.learn_every = params.learn_every #Every how many collect steps to train\n",
    "        self.sync_period = 2500*self.learn_every #COPY ONLINE NETWORK TO OFFLINE\n",
    "        self.rewards_file = rewards_file\n",
    "\n",
    "        \n",
    "    def log_episode(self):\n",
    "        self.episode_rewards.append(self.current_episode_reward)\n",
    "        self.current_episode_reward = 0.0\n",
    "\n",
    "    def log_period(self, episode, epsilon, step):\n",
    "        self.moving_average_episode_rewards.append(np.round(np.mean(self.episode_rewards[-checkpoint_period:]), 3))\n",
    "        self.max_average_episode_rewards.append(np.round(np.max(self.episode_rewards[-checkpoint_period:]), 3))\n",
    "        print(f\"Episode {episode} | Step {step} | Exploration rate {epsilon:.2f} \\\n",
    "        | Mean Reward {self.moving_average_episode_rewards[-1]} \\\n",
    "        | Max Reward {self.max_average_episode_rewards[-1]}\")\n",
    "\n",
    "        with open(os.path.join(self.save_directory,self.rewards_file), 'w') as f:\n",
    "            for i, (reward1,reward2) in enumerate(zip(self.moving_average_episode_rewards,self.max_average_episode_rewards)):\n",
    "                f.write(\"%d %.1f %.1f\\n\" %(i,reward1,reward2))\n",
    "\n",
    "\n",
    "    def remember(self, state, next_state, action, reward, done):\n",
    "        self.memory.append((torch.tensor(state.__array__()), torch.tensor(next_state.__array__()),\n",
    "                            torch.tensor([action]), torch.tensor([reward]), torch.tensor([done])))\n",
    "\n",
    "        \n",
    "    def experience_replay(self, step_reward):\n",
    "        self.current_episode_reward += step_reward\n",
    "        \n",
    "        if self.current_step%self.sync_period == 0: #Copy network pieces if time\n",
    "            self.net.target_conv.load_state_dict(self.net.conv.state_dict())\n",
    "            self.net.target_linear_adv.load_state_dict(self.net.linear_adv.state_dict())\n",
    "            self.net.target_linear_val.load_state_dict(self.net.linear_val.state_dict())\n",
    "            \n",
    "        if len(self.memory)<self.burn_in: #Don't train till have collected enough data\n",
    "            if(self.current_step%100==0): \n",
    "                print(\"Collecting data without training on step %d\" %self.current_step)\n",
    "            return\n",
    "\n",
    "        if self.current_step%self.learn_every !=0 : #Learn every N steps\n",
    "            return\n",
    "        \n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "        q_estimate = self.net(state.cuda(), model=\"online\")[np.arange(0, self.batch_size), action.cuda()]\n",
    "        with torch.no_grad():\n",
    "            action_preds = self.net(next_state.cuda(), model=\"online\")\n",
    "            best_action = torch.argmax(action_preds, dim=1)\n",
    "            \n",
    "            next_q = self.net(next_state.cuda(), model=\"target\")[np.arange(0, self.batch_size), best_action]\n",
    "            q_target = (reward.cuda() + (1 - done.cuda().float()) * self.gamma * next_q).float()\n",
    "        loss = self.loss(q_estimate, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        \n",
    "    def recall(self):\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*random.sample(self.memory, self.batch_size)))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            action_values = self.net(torch.tensor(state.__array__()).cuda().unsqueeze(0), model=\"online\")\n",
    "            action = torch.argmax(action_values, dim=1).item()\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "        self.current_step += 1\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def load_checkpoint(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.net.load_state_dict(checkpoint['model'])\n",
    "        self.exploration_rate = checkpoint['exploration_rate']\n",
    "\n",
    "        \n",
    "    def save_checkpoint(self):\n",
    "        filename = os.path.join(self.save_directory, 'checkpoint_{}.pth'.format(episode))\n",
    "        torch.save(dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))\n",
    "        \n",
    "        #Save experience replay for checkpointing\n",
    "        if params.save_memory: \n",
    "            filename = os.path.join(self.save_directory, 'memory_{}.pkl'.format(episode))\n",
    "            p_file = open(filename, 'wb')\n",
    "            pickle.dump(self.memory, p_file)\n",
    "            p_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened layer size is 6400\n",
      "Flattened layer size is 6400\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 84, 84]           1,184\n",
      "              ReLU-2           [-1, 32, 84, 84]               0\n",
      "         MaxPool2d-3           [-1, 32, 42, 42]               0\n",
      "            Conv2d-4           [-1, 64, 42, 42]          18,496\n",
      "              ReLU-5           [-1, 64, 42, 42]               0\n",
      "         MaxPool2d-6           [-1, 64, 21, 21]               0\n",
      "            Conv2d-7           [-1, 64, 21, 21]          36,928\n",
      "              ReLU-8           [-1, 64, 21, 21]               0\n",
      "         MaxPool2d-9           [-1, 64, 10, 10]               0\n",
      "          Flatten-10                 [-1, 6400]               0\n",
      "           Linear-11                  [-1, 512]       3,277,312\n",
      "             ReLU-12                  [-1, 512]               0\n",
      "           Linear-13                    [-1, 4]           2,052\n",
      "           Linear-14                  [-1, 512]       3,277,312\n",
      "             ReLU-15                  [-1, 512]               0\n",
      "           Linear-16                    [-1, 1]             513\n",
      "================================================================\n",
      "Total params: 6,613,797\n",
      "Trainable params: 6,613,797\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 6.36\n",
      "Params size (MB): 25.23\n",
      "Estimated Total Size (MB): 31.70\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Summary of model    \n",
    "from torchsummary import summary\n",
    "from DDQNSolver import DDQNSolver\n",
    "model = DDQNSolver(env.action_space.n, env.observation_space.shape)\n",
    "print(summary(model,(4,84,84),device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load checkpoint\n",
    "checkpoint_period = params.checkpoint_period\n",
    "episode = params.episode\n",
    "max_episodes = params.max_episodes    \n",
    "\n",
    "\n",
    "#Log rewards and network weights\n",
    "rewards_file = params.rewards_file\n",
    "save_directory = params.save_directory\n",
    "if not os.path.exists (os.getcwd() + '/' + save_directory):\n",
    "    os.mkdir(os.getcwd() + '/' + save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened layer size is 6400\n",
      "Flattened layer size is 6400\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3867467/590839096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Initialize agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m agent = DDQNAgent(action_dim=env.action_space.n, obs_dim = env.observation_space.shape,\n\u001b[0m\u001b[1;32m      3\u001b[0m                   save_directory=save_directory, rewards_file=rewards_file)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Start from checkpoint?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3867467/82899471.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, action_dim, obs_dim, save_directory, rewards_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDQNSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_rate_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_rate_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/SB3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \"\"\"\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/SB3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/SB3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/SB3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/SB3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \"\"\"\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/SB3/lib/python3.9/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "#Initialize agent\n",
    "agent = DDQNAgent(action_dim=env.action_space.n, obs_dim = env.observation_space.shape,\n",
    "                  save_directory=save_directory, rewards_file=rewards_file)\n",
    "\n",
    "if params.load_checkpoint is not None: #Start from checkpoint?\n",
    "    agent.load_checkpoint(save_directory + \"/\" + params.load_checkpoint) #Load weights\n",
    "    memory_file = save_directory + \"/memory_%d.pkl\" %episode #Load experience replay deque\n",
    "    agent.memory = pickle.load( open( memory_file, \"rb\" ) )\n",
    "    agent.current_step = episode * env.Nvacs #Load number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n",
      "Collecting data without training on step 100\n",
      "Collecting data without training on step 200\n",
      "Collecting data without training on step 300\n",
      "Episode 100 | Step 18295 | Exploration rate 0.91         | Mean Reward 1.26         | Max Reward 4.0\n",
      "Checkpoint saved to 'v2/checkpoint_100.pth'\n",
      "Episode 200 | Step 36365 | Exploration rate 0.83         | Mean Reward 1.22         | Max Reward 10.0\n",
      "Checkpoint saved to 'v2/checkpoint_200.pth'\n",
      "Episode 300 | Step 56132 | Exploration rate 0.76         | Mean Reward 1.6         | Max Reward 8.0\n",
      "Checkpoint saved to 'v2/checkpoint_300.pth'\n",
      "Episode 400 | Step 74782 | Exploration rate 0.69         | Mean Reward 1.33         | Max Reward 7.0\n",
      "Checkpoint saved to 'v2/checkpoint_400.pth'\n",
      "Episode 500 | Step 92807 | Exploration rate 0.63         | Mean Reward 1.09         | Max Reward 5.0\n",
      "Checkpoint saved to 'v2/checkpoint_500.pth'\n",
      "Episode 600 | Step 112152 | Exploration rate 0.57         | Mean Reward 1.46         | Max Reward 6.0\n",
      "Checkpoint saved to 'v2/checkpoint_600.pth'\n",
      "Episode 700 | Step 133111 | Exploration rate 0.51         | Mean Reward 2.2         | Max Reward 8.0\n",
      "Checkpoint saved to 'v2/checkpoint_700.pth'\n",
      "Episode 800 | Step 153063 | Exploration rate 0.47         | Mean Reward 1.93         | Max Reward 10.0\n",
      "Checkpoint saved to 'v2/checkpoint_800.pth'\n",
      "Episode 900 | Step 171904 | Exploration rate 0.42         | Mean Reward 1.29         | Max Reward 5.0\n",
      "Checkpoint saved to 'v2/checkpoint_900.pth'\n",
      "Episode 1000 | Step 197047 | Exploration rate 0.37         | Mean Reward 3.07         | Max Reward 11.0\n",
      "Checkpoint saved to 'v2/checkpoint_1000.pth'\n",
      "Episode 1100 | Step 220020 | Exploration rate 0.33         | Mean Reward 2.73         | Max Reward 12.0\n",
      "Checkpoint saved to 'v2/checkpoint_1100.pth'\n",
      "Episode 1200 | Step 248408 | Exploration rate 0.29         | Mean Reward 5.4         | Max Reward 11.0\n",
      "Checkpoint saved to 'v2/checkpoint_1200.pth'\n",
      "Episode 1300 | Step 281427 | Exploration rate 0.24         | Mean Reward 7.19         | Max Reward 14.0\n",
      "Checkpoint saved to 'v2/checkpoint_1300.pth'\n",
      "Episode 1400 | Step 319962 | Exploration rate 0.20         | Mean Reward 8.99         | Max Reward 21.0\n",
      "Checkpoint saved to 'v2/checkpoint_1400.pth'\n",
      "Episode 1500 | Step 364339 | Exploration rate 0.16         | Mean Reward 9.82         | Max Reward 22.0\n",
      "Checkpoint saved to 'v2/checkpoint_1500.pth'\n",
      "Episode 1600 | Step 423367 | Exploration rate 0.12         | Mean Reward 17.36         | Max Reward 34.0\n",
      "Checkpoint saved to 'v2/checkpoint_1600.pth'\n",
      "Episode 1700 | Step 489514 | Exploration rate 0.10         | Mean Reward 21.94         | Max Reward 44.0\n",
      "Checkpoint saved to 'v2/checkpoint_1700.pth'\n",
      "Episode 1800 | Step 553094 | Exploration rate 0.10         | Mean Reward 20.91         | Max Reward 35.0\n",
      "Checkpoint saved to 'v2/checkpoint_1800.pth'\n",
      "Episode 1900 | Step 614368 | Exploration rate 0.10         | Mean Reward 18.54         | Max Reward 30.0\n",
      "Checkpoint saved to 'v2/checkpoint_1900.pth'\n",
      "Episode 2000 | Step 680777 | Exploration rate 0.10         | Mean Reward 22.63         | Max Reward 44.0\n",
      "Checkpoint saved to 'v2/checkpoint_2000.pth'\n",
      "Episode 2100 | Step 749387 | Exploration rate 0.10         | Mean Reward 23.35         | Max Reward 41.0\n",
      "Checkpoint saved to 'v2/checkpoint_2100.pth'\n",
      "Episode 2200 | Step 818547 | Exploration rate 0.10         | Mean Reward 22.81         | Max Reward 42.0\n",
      "Checkpoint saved to 'v2/checkpoint_2200.pth'\n",
      "Episode 2300 | Step 895388 | Exploration rate 0.10         | Mean Reward 27.72         | Max Reward 51.0\n",
      "Checkpoint saved to 'v2/checkpoint_2300.pth'\n",
      "Episode 2400 | Step 964139 | Exploration rate 0.10         | Mean Reward 23.03         | Max Reward 61.0\n",
      "Checkpoint saved to 'v2/checkpoint_2400.pth'\n",
      "Episode 2500 | Step 1036648 | Exploration rate 0.10         | Mean Reward 27.42         | Max Reward 65.0\n",
      "Checkpoint saved to 'v2/checkpoint_2500.pth'\n",
      "Episode 2600 | Step 1108571 | Exploration rate 0.10         | Mean Reward 27.35         | Max Reward 52.0\n",
      "Checkpoint saved to 'v2/checkpoint_2600.pth'\n",
      "Episode 2700 | Step 1183691 | Exploration rate 0.10         | Mean Reward 30.41         | Max Reward 60.0\n",
      "Checkpoint saved to 'v2/checkpoint_2700.pth'\n",
      "Episode 2800 | Step 1252454 | Exploration rate 0.10         | Mean Reward 27.79         | Max Reward 49.0\n",
      "Checkpoint saved to 'v2/checkpoint_2800.pth'\n",
      "Episode 2900 | Step 1318193 | Exploration rate 0.10         | Mean Reward 26.52         | Max Reward 51.0\n",
      "Checkpoint saved to 'v2/checkpoint_2900.pth'\n",
      "Episode 3000 | Step 1386891 | Exploration rate 0.10         | Mean Reward 29.83         | Max Reward 68.0\n",
      "Checkpoint saved to 'v2/checkpoint_3000.pth'\n",
      "Episode 3100 | Step 1462195 | Exploration rate 0.10         | Mean Reward 35.05         | Max Reward 64.0\n",
      "Checkpoint saved to 'v2/checkpoint_3100.pth'\n",
      "Episode 3200 | Step 1533177 | Exploration rate 0.10         | Mean Reward 33.58         | Max Reward 68.0\n",
      "Checkpoint saved to 'v2/checkpoint_3200.pth'\n",
      "Episode 3300 | Step 1602890 | Exploration rate 0.10         | Mean Reward 32.96         | Max Reward 69.0\n",
      "Checkpoint saved to 'v2/checkpoint_3300.pth'\n",
      "Episode 3400 | Step 1671698 | Exploration rate 0.10         | Mean Reward 36.56         | Max Reward 97.0\n",
      "Checkpoint saved to 'v2/checkpoint_3400.pth'\n",
      "Episode 3500 | Step 1740932 | Exploration rate 0.10         | Mean Reward 35.67         | Max Reward 100.0\n",
      "Checkpoint saved to 'v2/checkpoint_3500.pth'\n",
      "Episode 3600 | Step 1813767 | Exploration rate 0.10         | Mean Reward 36.69         | Max Reward 121.0\n",
      "Checkpoint saved to 'v2/checkpoint_3600.pth'\n",
      "Episode 3700 | Step 1889308 | Exploration rate 0.10         | Mean Reward 36.05         | Max Reward 83.0\n",
      "Checkpoint saved to 'v2/checkpoint_3700.pth'\n",
      "Episode 3800 | Step 1967992 | Exploration rate 0.10         | Mean Reward 40.74         | Max Reward 86.0\n",
      "Checkpoint saved to 'v2/checkpoint_3800.pth'\n",
      "Episode 3900 | Step 2045719 | Exploration rate 0.10         | Mean Reward 42.17         | Max Reward 191.0\n",
      "Checkpoint saved to 'v2/checkpoint_3900.pth'\n",
      "Episode 4000 | Step 2118566 | Exploration rate 0.10         | Mean Reward 36.26         | Max Reward 81.0\n",
      "Checkpoint saved to 'v2/checkpoint_4000.pth'\n",
      "Episode 4100 | Step 2194418 | Exploration rate 0.10         | Mean Reward 43.04         | Max Reward 147.0\n",
      "Checkpoint saved to 'v2/checkpoint_4100.pth'\n",
      "Episode 4200 | Step 2267986 | Exploration rate 0.10         | Mean Reward 41.35         | Max Reward 340.0\n",
      "Checkpoint saved to 'v2/checkpoint_4200.pth'\n",
      "Episode 4300 | Step 2342340 | Exploration rate 0.10         | Mean Reward 42.69         | Max Reward 239.0\n",
      "Checkpoint saved to 'v2/checkpoint_4300.pth'\n",
      "Episode 4400 | Step 2411702 | Exploration rate 0.10         | Mean Reward 38.02         | Max Reward 98.0\n",
      "Checkpoint saved to 'v2/checkpoint_4400.pth'\n",
      "Episode 4500 | Step 2490659 | Exploration rate 0.10         | Mean Reward 48.14         | Max Reward 273.0\n",
      "Checkpoint saved to 'v2/checkpoint_4500.pth'\n",
      "Episode 4600 | Step 2575526 | Exploration rate 0.10         | Mean Reward 47.69         | Max Reward 192.0\n",
      "Checkpoint saved to 'v2/checkpoint_4600.pth'\n",
      "Episode 4700 | Step 2651881 | Exploration rate 0.10         | Mean Reward 43.5         | Max Reward 153.0\n",
      "Checkpoint saved to 'v2/checkpoint_4700.pth'\n",
      "Episode 4800 | Step 2725475 | Exploration rate 0.10         | Mean Reward 41.02         | Max Reward 187.0\n",
      "Checkpoint saved to 'v2/checkpoint_4800.pth'\n",
      "Episode 4900 | Step 2806132 | Exploration rate 0.10         | Mean Reward 47.6         | Max Reward 339.0\n",
      "Checkpoint saved to 'v2/checkpoint_4900.pth'\n",
      "Episode 5000 | Step 2883158 | Exploration rate 0.10         | Mean Reward 44.22         | Max Reward 190.0\n",
      "Checkpoint saved to 'v2/checkpoint_5000.pth'\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "while episode<max_episodes:\n",
    "    state = env.reset()\n",
    "    cstep = 0 #Keep track of steps in this episode\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        if cstep==0: \n",
    "            action = np.random.randint(agent.action_dim) \n",
    "            #Take first action randomly so start from different places\n",
    "        cstep+=1\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.remember(state, next_state, action, reward, done)\n",
    "        agent.experience_replay(reward)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            cstep = 0\n",
    "            episode += 1\n",
    "            agent.log_episode()\n",
    "            if episode % checkpoint_period == 0:\n",
    "                agent.log_period(episode=episode, epsilon=agent.exploration_rate, step=agent.current_step)\n",
    "                agent.save_checkpoint()\n",
    "                plt.imshow(state.__array__().squeeze().T, origin='lower')\n",
    "                plt.savefig(save_directory + \"/img%d.png\" %episode)\n",
    "                plt.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8632b2696f11414c6bc4e32ed121b733bde58e38550c0199c84c0b8592cb6670"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
